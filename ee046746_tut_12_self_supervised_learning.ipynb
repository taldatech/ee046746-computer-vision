{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <img src=\"https://img.icons8.com/bubbles/50/000000/mind-map.png\" style=\"height:50px;display:inline\"> EE 046746 - Technion - Computer Vision\n",
    "---\n",
    "\n",
    "#### <a href=\"https://taldatech.github.io\">Tal Daniel</a> (adapted from Tal's Tutorials in ECE046211 - Deep Learning)\n",
    "\n",
    "## Tutorial 12 - Self-Supervised Learning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/checklist.png\" style=\"height:50px;display:inline\"> Agenda\n",
    "---\n",
    "* [Representation and Self-Supervised Learning](#-Representation-and-Self-Supervised-Learning)\n",
    "* [Autoencoders](#-Deep-Unsupervised-Learning---Deep-Autoencoders)\n",
    "* [Self-Supervised Learning](#-Self-Supervised-Learning)\n",
    "  * [Corrupted Version Reconstruction & Visual Common Sense Tasks](#-Corrupted-Version-Reconstruction-&-Visual-Common-Sense-Tasks)\n",
    "  * [Contrastive Methods](#-Contrastive-Learning)\n",
    "    * [Simple Framework for Contrastive Learning of Visual Representations (SimCLR)](#-Simple-Framework-for-Contrastive-Learning-of-Visual-Representations-(SimCLR))\n",
    "    * [Using the Learned Representation for Downstream Tasks](#-Using-the-Learned-Representation-for-Downstream-Tasks)\n",
    "    * [Momentum Contrast (MoCo)](#-Momentum-Contrast-(MoCo))\n",
    "    * [Contrastive Predictive Coding (CPC)](#-Contrastive-Predictive-Coding-(CPC))\n",
    "    * [Performance Comparison](#-Performance-Comparison)\n",
    "* [Recommended Videos](#-Recommended-Videos)\n",
    "* [Credits](#-Credits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <img src=\"https://img.icons8.com/color/96/000000/self-esteem.png\" style=\"height:50px;display:inline\"> Representation and Self-Supervised Learning\n",
    "---\n",
    "* Data is usually abundant and cheap, it's the labels that are expensive.\n",
    "  * Can we use the unlabeled data to gain knowledge that is usable for downstream supervised tasks?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Maybe we can learn rich and useful features from raw unlabeled data\n",
    "  * We can learn a representation of the data!\n",
    "<img src=\"./assets/rep_data.png\" style=\"height:250px\">\n",
    "* **The way we represent the data has a great impact on the performance and compelxity**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* What are the various general tasks that can be used to learn representations from unlabelled data?\n",
    "  * **Deep Unsupervised Learning** - learn representations without lables, subset of deep learning, which is a subset of representation learning, which is a subset of machine learning.\n",
    "  * **Self-supervised Learning** - often used interchangeably with unsupervised learning. Self-supervised: **create your own supervision through pretext tasks**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/color/96/000000/code.png\" style=\"height:50px;display:inline\"> Deep Unsupervised Learning - Deep Autoencoders\n",
    "---\n",
    "* **Motivation:** Most of the natural data is high-dimensional, such as images. Consider the MNIST (hand-written digits) dataset, where each image has $28x28=784$ pixels, which means it can be represented by a vector of length 784. \n",
    "    * But do we really need 784 values to represent a digit? The answer is probably no. We believe that the data lies on a low-dimensional manifold which is enough to describe the observations. In the case of MNIST, we known that there are 10 digits - so we can represent the digits as one-hot vectors, which means we only need 10 dimensions.\n",
    "    * So we can **encode** high-dimensional observations in a low-dimensional space.\n",
    "    * But how can we learn meaningful low-dimensional representations? \n",
    "    * The general idea is to reconstruct or, **decode** the low-dimensional representation to the high-dimensional representation, and use the reconstruction error to learn the best representations. This is the core idea behind **autoencoders**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<img src=\"./assets/MnistExamples.png\" style=\"height:250px\">\n",
    "\n",
    "* Image from <a href=\"https://en.wikipedia.org/wiki/MNIST_database\">Wikipedia</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* **Autoencoders** - models which take data as input and discover some latent state representation of that data. The input data is converted into an encoding vector where each dimension represents some learned attribute about the data. The most important detail to grasp here is that our encoder network is outputting a single value for each encoding dimension. The decoder network then subsequently takes these values and attempts to recreate the original input. Autoencoders have **three parts**: an encoder, a decoder, and a 'loss' function that maps one to the other. For the simplest autoencoders - the sort that compress and then reconstruct the original inputs from the compressed representation - we can think of the 'loss' as describing the amount of information lost in the process of reconstruction.\n",
    "        \n",
    "<img src=\"./assets/autoencoder_1.jpeg\" style=\"height:250px\">\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/cute-clipart/64/000000/task.png\" style=\"height:50px;display:inline\"> Self-Supervised Learning\n",
    "---\n",
    "* A version of unsupervised learning where **data provides the supervision**.\n",
    "* **Idea**: withhold some part of the data and then task a neural network to predict it from the remaining parts.\n",
    "* Details decide what proxy loss or pretext task the network tries to solve, and depending on the quality of the task, good semantic features can be obtained without actual labels.\n",
    "* Advantages over supervised learning:\n",
    "    * Large cost of producing a new dataset for each task (prepare labeling manuals, categories, hiring humans, creating GUIs, storage pipelines, etc).\n",
    "    * Good supervision may not be cheap (e.g., medicine, legal).\n",
    "    * Take advantage of vast amount of unlabeled data on the Internet (images, videos, language)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/cute-clipart/64/000000/console.png\" style=\"height:50px;display:inline\"> Self-Supervised Learning Methods\n",
    "---\n",
    "* *Reconstruct from a corrupted (or partial) version*\n",
    "    * Denoising Autoencoders - \"Withholded\" data: Clean image\n",
    "    * In-painting -\n",
    "    * Colorization, Split-Brain Autoencoder\n",
    "* *Visual common sense tasks*\n",
    "    * Relative patch prediction\n",
    "    * Jigsaw puzzles\n",
    "    * Rotation prediction\n",
    "* **Contrastive Learning** (our focus)\n",
    "    * word2vec\n",
    "    * Contrastive Predictive Coding (CPC)\n",
    "    * Instance Discrimination\n",
    "    * Simple Framework for Contrastive Learning of Visual Representations (SimCLR), Momentum Contrast (MoCo), Bootstrap Your Own Latent (BYOL)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/clouds/64/000000/white-noise.png\" style=\"height:50px;display:inline\"> Corrupted Version Reconstruction & Visual Common Sense Tasks\n",
    "---\n",
    "**Code Demos** - <a href=\"https://colab.research.google.com/github/rll/deepul/blob/master/demos/lecture7_selfsupervised_demos.ipynb\">Self-Supervised Learning Demos</a>\n",
    "\n",
    "* **Context Encoder** - Try to predict a hidden mask in the image\n",
    "  * <img src=\"./assets/context_encoder.PNG\" style=\"height:250px\">\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* The reconstruction is mediocre\n",
    "\n",
    "<img src=\"./assets/context_encoder_res.png\" style=\"height:250px\">\n",
    "\n",
    "* We care about the learned representation!\n",
    "  * Specifically - is it useful for a downstream task?\n",
    "\n",
    "\n",
    "| Top 1 Accuracy on CIFAR-10| Top 5 Accuracy CIFAR-10|\n",
    "|--|--|\n",
    "|45.77|90.29|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Did it learn anything?\n",
    "  * We can look the the nearest neighbors in the latent space to see if that's the case.\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"./assets/context_encoder_NN1.png\" style=\"height:250px\"> </td>\n",
    "<td> <img src=\"./assets/context_encoder_NN2.png\" style=\"height:250px\"> </td>\n",
    "</tr></table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* **Rotation Prediction** - Try to predict the rotation \"class\" of a given image.\n",
    "<img src=\"./assets/rotation_prediction.png\" style=\"height:400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* What about the learned representations?\n",
    "\n",
    "| | Top 1 Accuracy on CIFAR-10| Top 5 Accuracy on CIFAR-10|\n",
    "|--|--|--|\n",
    "| Context Encoder |45.77|90.29|\n",
    "| **Rotation Prediction** |79.91|99.12|\n",
    "\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"./assets/rotation_prediction_NN1.png\" style=\"height:250px\"> </td>\n",
    "<td> <img src=\"./assets/rotation_prediction_NN2.png\" style=\"height:250px\"> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/plasticine/100/000000/protect-from-magnetic-field.png\" style=\"height:50px;display:inline\"> Contrastive Learning\n",
    "---\n",
    "* Contrastive learning is an approach to formulate the task of **finding similar and dissimilar things for a ML model (basically what classification does when given labels)**. \n",
    "* Contrastive methods, as the name implies, learn representations by contrasting **positive and negative** examples. \n",
    "* Using this approach, one can train a machine learning model to classify between similar and dissimilar images.\n",
    "<img src=\"./assets/contrastive_1.png\" style=\"height:150px\">\n",
    "<img src=\"./assets/contrastive_puzzle.gif\" style=\"height:200px\">\n",
    "\n",
    "* <a href=\"https://analyticsindiamag.com/contrastive-learning-self-supervised-ml\">Image Source</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "* More formally, for any data point $x$, contrastive methods aim to learn an encoder $f$ such that: \n",
    "    * $x^+$ is a data point similar to $x$, referred to as a *positive* sample.\n",
    "    * $x^−$ is a data point dissimilar to $x$, referred to as a *negative* sample.\n",
    "    * The **score function** is a metric that measures the similarity between two features: $$score(f(x), f(x^+))  >>  score(f(x), f(x^-))$$\n",
    "    \n",
    "    \n",
    "    \n",
    "* How can we sample \"similar\" or \"different\" images?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* The most common loss function to implement the score paradigm is **InfoNCE** loss, which looks similar to softmax.\n",
    "\n",
    "<img src=\"./assets/infonce_loss.png\" style=\"height:100px\">\n",
    "\n",
    "* The denominator terms consist of one positive sample, and N−1 negative samples. \n",
    "\n",
    "* Compare this to softmax: $$Softmax(y_i)=\\frac{e^{y_i}}{\\sum_{j=1}^{M}e^{y_j}},\\quad i\\in[1,...,M],y\\in\\mathbb{R}^M$$\n",
    "  * Where $y_i$ is the angle between the normalized representation of the images $u^{T}w=||u||||w||\\cos{\\angle(u,w)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/nolan/64/collapse-arrow.png\" style=\"height:50px;display:inline\"> Simple Framework for Contrastive Learning of Visual Representations (SimCLR)\n",
    "---\n",
    "* <a href=\"https://arxiv.org/abs/2002.05709\">**Simple Framework for Contrastive Learning of Visual Representations (SimCLR)**</a> is a framework for contrastive learning of *visual* representations. \n",
    "* It learns representations by maximizing agreement between differently augmented views of the same data example via a contrastive loss in the latent space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"./assets/simclr.png\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* A **stochastic data augmentation module** that transforms any given data example randomly resulting in two correlated views of the same example, denoted $\\tilde{x}_i$ and $\\tilde{x}_j$, which is considered a **positive pair**.\n",
    "* SimCLR sequentially applies three simple augmentations: random cropping followed by resize back to the original size, random color distortions, and random Gaussian blur. The authors find **random crop and color distortion** is crucial to achieve good performance.\n",
    "* A neural network base encoder $f(\\cdot)$ that extracts **representation vectors** from augmented data examples. The framework allows various choices of the network architecture without any constraints. \n",
    "    * For simplicity ResNet is used to obtain $h_i = f(\\tilde{x}_i)\\in \\mathcal{R}^d$ where $h_i$ is the output after the average pooling layer.\n",
    "* A small neural network projection head $g(\\cdot)$ that maps representations to the space where contrastive loss is applied. \n",
    "* MLP with one hidden layer is used to obtain $z_i=g(h_i)$.\n",
    "* **The authors find it beneficial to define the contrastive loss on $z_i$’s rather than $h_i$’s**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* A minibatch of $N$ examples is randomly sampled and the contrastive prediction task is defined on pairs of augmented examples derived from the minibatch, resulting in $2N$ data points. \n",
    "* Negative examples are not sampled explicitly. Instead, given a positive pair, the other $2(N-1)$ augmented examples within a minibatch are treated as negative examples. \n",
    "* A NT-Xent (the normalized temperature-scaled cross entropy loss) loss function is used: $$\\ell_{i,j} = -\\log{\\frac{\\exp(\\text{sim}(z_i, z_j)/\\tau)}{\\sum_{k=1}^{2N}\\mathbb{1}_{[k\\neq i]}\\exp(\\text{sim}(z_i, z_k)/\\tau)}}$$\n",
    "where $\\text{sim}(z_i, z_j) = \\frac{z_i^Tz_j}{\\left\\Vert z_i \\right\\Vert \\left\\Vert z_j \\right\\Vert}$\n",
    "\n",
    "* <a href=\"https://github.com/sthalles/SimCLR\">PyTorch Code</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"./assets/simclr_anim.gif\" style=\"height:350px\">\n",
    "\n",
    "* <a href=\"https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html\">Image Source</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Why can't we just use the positive samples?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "* How likely is our negative sample to actually be negative?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"./assets/SSCL_probelm.png\" style=\"height:350px\"> \n",
    "\n",
    "\n",
    "* <a href=\"https://www.v7labs.com/blog/contrastive-learning-guide\">Image Source</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* What about the representations?\n",
    "\n",
    "| | Top 1 Accuracy on CIFAR-10| Top 5 Accuracy on CIFAR-10| Top 1 Accuracy on Imagenet| Top 5 Accuracy on Imagenet|\n",
    "|--|--|--|--|--|\n",
    "| Context Encoder |45.77|90.29|||\n",
    "| Rotation Prediction |79.91|99.12|||\n",
    "| **SimCLR** |92.84|99.86|69.3|89.0|\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"./assets/simclr_NN1.png\" style=\"height:250px\"> </td>\n",
    "<td> <img src=\"./assets/simclr_NN2.png\" style=\"height:250px\"> </td>\n",
    "</tr></table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/64/000000/knowledge-transfer.png\" style=\"height:50px;display:inline\"> Using the Learned Representation for Downstream Tasks\n",
    "---\n",
    "\n",
    "* Is classification the only downstream task our learned representation can help?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Example: **Segmentation** on PASCAL VOC2012 \n",
    "\n",
    "<img src=\"./assets/self_sup_seg.png\" style=\"height:400px\">\n",
    "\n",
    "\n",
    "* Images from <a href=\"https://colab.research.google.com/github/rll/deepul/blob/master/demos/lecture7_selfsupervised_demos.ipynb\">Berkeley's Deep Unsupervised Learning Course</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/officel/80/000000/gyroscope.png\" style=\"height:50px;display:inline\"> Momentum Contrast (MoCo)\n",
    "---\n",
    "* <a href=\"https://arxiv.org/abs/1911.05722\">**Momentum Contrast (MoCo)**</a> is a self-supervised learning algorithm with a contrastive loss.\n",
    "* Contrastive loss methods can be thought of as **building dynamic dictionaries**. \n",
    "* The **\"keys\" (tokens)** in the dictionary are sampled from data (e.g., images or patches) and are represented by an encoder network. \n",
    "* Unsupervised learning trains encoders (by minimizing a contrastive loss) to perform dictionary look-up: an encoded “query” should be similar to its matching key and dissimilar to others.\n",
    "* In MoCo, we maintain the dictionary as a queue of data samples: the encoded representations of the current mini-batch are enqueued, and the oldest are dequeued.\n",
    "*  The queue decouples the dictionary size from the mini-batch size, allowing it to be large.\n",
    "* Moreover, as the dictionary keys come from the preceding several mini-batches, a slowly progressing key encoder, implemented as a momentum-based moving average of the query encoder, is proposed to maintain consistency.\n",
    "* <a href=\"https://github.com/facebookresearch/moco\">PyTorch Code</a>\n",
    "    * <a href=\"https://colab.research.google.com/github/facebookresearch/moco/blob/colab-notebook/colab/moco_cifar10_demo.ipynb\">Colab Demo</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* The positive samples part remains the same as in SimCLR.\n",
    "* For the negative samples - We build a **momentum encoder**\n",
    "  * This encoder's architecture is the same as the normal encoder, but its weights are an **exponential moving average**: \n",
    "  $$W_{\\text{momentum}\\newline\\text{encoder}}^k =\\beta \\cdot W_{\\text{momentum}\\newline\\text{encoder}}^{k-1} + (1-\\beta)\\cdot W_{\\text{encoder}}$$\n",
    "  * The momentum encoder doesn't have backpropegation -  so we can save memory and apply it on a large amount of negative samples\n",
    "\n",
    "<img src=\"./assets/moco.png\" style=\"height:350px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "| Short training| Top 1 Accuracy on Imagenet| Batch size|\n",
    "|--|--|--|\n",
    "| SimCLR |61.9| 256|\n",
    "| SimCLR |66.6| 8192|\n",
    "| **MoCo** |**60.6**|**256**|\n",
    "| MoCo v2|67.5|256|\n",
    "\n",
    "| Long training| Top 1 Accuracy on Imagenet| Batch size|\n",
    "|--|--|--|\n",
    "| SimCLR |69.3| 4096|\n",
    "| MoCo v2 |71.1|256|\n",
    "\n",
    "* Both MoCo and SimCLR can be categorized (along with other newer methods such as SwAV, BYOL, etc.) as Instance Based (Discrimination) Contrastive Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/pastel-glyph/64/000000/qr-code--v2.png\" style=\"height:50px;display:inline\"> Contrastive Predictive Coding (CPC)\n",
    "---\n",
    "* <a href=\"https://arxiv.org/abs/1807.03748\">**Contrastive Predictive Coding (CPC)**</a> learns self-supervised **representations** (Coding) by **predicting the future** (Predictive) in a learned *latent space* by using powerful autoregressive models that **Contrast** (Contrastive) \"right\" and \"wrong\" sequences. \n",
    "* The model uses a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"./assets/cpc.png\"  style=\"height:400px\">\n",
    "\n",
    "* <a href=\"https://github.com/davidtellez/contrastive-predictive-coding\">Image Source</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"./assets/cpc2.png\"  style=\"height:400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. A non-linear encoder $g_{enc}$ maps the input sequence of observations $x_t$ to a sequence of latent representations $z_t = g_{enc}(x_t)$, potentially with a lower resolution.\n",
    "2. An autoregressive model $g_{ar}$ summarizes all $z \\leq t$ in the latent space and produces a context latent representation $c_t=g_{ar}(z \\leq t)$.\n",
    "  * In the original paper they used a single sample window $k=1$ (Predict the future based on the last sample)\n",
    "3. Compute the InfoNCE loss between the future code $z_{t+k}$ and the predicted future code $\\hat{z}_{t+k}$ based on the context $c_t$\n",
    "  * Remember in InfoNCE we have $f(x)^{T}f(x_j)$, so here $f_k(x_{t+k}, c_t) = \\exp(z^T_{t+k}W_{k}c_{t})$\n",
    "    * $f$ is modeled to preserves the mutual information between $x_{t+k}$ and $c_t$ ($f_k(x_{t+k}, c_t) \\propto \\frac{\\mathbb{P}(x_{t+k}|c_t)}{\\mathbb{P}(x_{t+k}})$)\n",
    "    * $W_k$ are learned weights, $f$ can be unnormalized (does not have to integrate to 1)\n",
    "  * Negative samples are an incorrect \"future prediction\"\n",
    "* Any type of encoder and autoregressive can be used. \n",
    "    * For example: strided convolutional layers with RNN and GRUs.\n",
    "    \n",
    "* <a href=\"https://github.com/jefflai108/Contrastive-Predictive-Coding-PyTorch\">PyTorch Code</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* How can we \"predict the future\" in images?\n",
    "\n",
    "<img src=\"./assets/cpc_future.gif\"  style=\"height:300px\">\n",
    "\n",
    "* <a href=\"https://towardsdatascience.com/a-framework-for-contrastive-self-supervised-learning-and-designing-a-new-approach-3caab5d29619\">Image Source</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. Create a sequence from the image: Split the image into overlapping patches, and model rows of patches from top to bottom as a sequence.\n",
    "2. Use an encoder $g_{enc}$ that's fit for images (e.g. Resnet-50 architecture)\n",
    "3. Use an autoregressive model fit for images (e.g. PixelCNN) to create a context vector $c_t$ from the first $k$ rows.\n",
    "4. Compute the InfoNCE loss between the context $c_t$ and the predicted rows $z_{t+k}$\n",
    "  * Negative samples will be incorrect rows\n",
    "\n",
    "<img src=\"./assets/cpc_images.png\"  style=\"height:350px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/clouds/80/000000/performance-2.png\" style=\"height:50px;display:inline\"> Performance Comparison\n",
    "---\n",
    "Performance on ImageNet (linear evaluation) using ResNet-50 and ResNet200 (2×), compared to other unsupervised and supervised (Sup.) baselines:\n",
    "\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"./assets/self_supervised_perf1.png\" style=\"height:350px\"> </td>\n",
    "<td> <img src=\"./assets/self_supervised_perf2.png\" style=\"height:350px\"> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/video-playlist.png\" style=\"height:50px;display:inline\"> Recommended Videos\n",
    "---\n",
    "#### <img src=\"https://img.icons8.com/cute-clipart/64/000000/warning-shield.png\" style=\"height:30px;display:inline\"> Warning!\n",
    "* These videos do not replace the lectures and tutorials.\n",
    "* Please use these to get a better understanding of the material, and not as an alternative to the written material.\n",
    "\n",
    "#### Video By Subject\n",
    "\n",
    "* General Self-Supervised Learning - <a href=\"https://www.youtube.com/watch?v=dMUes74-nYY\">Lecture 7 Self-Supervised Learning - UC Berkeley Spring 2020 - CS294-158 Deep Unsupervised Learning</a>\n",
    "* SimCLR - <a href=\"https://www.youtube.com/watch?v=APki8LmdJwY\">SimCLR Explained!</a>\n",
    "* MoCo - <a href=\"https://www.youtube.com/watch?v=LvHwBQF14zs\">Momentum Contrastive Learning</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## <img src=\"https://img.icons8.com/dusk/64/000000/prize.png\" style=\"height:50px;display:inline\"> Credits\n",
    "---\n",
    "* EE 046211 Winter 22 - Original Tutorial - <a href=\"https://taldatech.github.io/\">Tal Daniel</a> \n",
    "* EE 046746 Spring 22 - <a href=\"https://github.com/HilaManor\">Hila Manor</a>\n",
    "* Icons made by <a href=\"https://www.flaticon.com/authors/becris\" title=\"Becris\">Becris</a> from <a href=\"https://www.flaticon.com/\" title=\"Flaticon\">www.flaticon.com</a>\n",
    "* Icons from <a href=\"https://icons8.com/\">Icons8.com</a> - https://icons8.com\n",
    "* <a href=\"https://sites.google.com/view/berkeley-cs294-158-sp20/home\">Berkeley's CS294-158-SP20-Deep Unsupervised Learning</a>\n",
    "* <a href=\"http://cs231n.stanford.edu/2021/\">Stanford's CS2331n (Spring 2021) -Convolutional Neural Networks for Visual Recognition</a>\n",
    "* <a href=\"https://paperswithcode.com/method/contrastive-predictive-coding\">Contrastive Predictive Coding</a>\n",
    "* <a href=\"https://paperswithcode.com/method/simclr\"> Simple Framework for Contrastive Learning of Visual Representations (SimCLR)</a>\n",
    "* <a href=\"https://paperswithcode.com/method/moco\">Momentum Contrast</a>\n",
    "* <a href=\"https://arxiv.org/pdf/2009.00104.pdf\">A Framework For Contrastive Self-Supervised Learning And Designing A New Approach</a>\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
