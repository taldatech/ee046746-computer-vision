{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <img src=\"https://img.icons8.com/bubbles/100/000000/3d-glasses.png\" style=\"height:50px;display:inline\"> EE 046746 - Technion - Computer Vision\n",
    "--- \n",
    "#### <a href=\"https://eliasnehme.github.io/\">Elias Nehme</a>\n",
    "\n",
    "## Tutorial 02 - Probabilistic Discriminative Learning\n",
    "\n",
    "---\n",
    "<img src=\"https://miro.medium.com/max/1400/1*_ajkhD8gbCBwVFOjNfEAzw.png\" width=\"800\">\n",
    "\n",
    "* <a href=\"https://medium.com/@jordi299/about-generative-and-discriminative-models-d8958b67ad32\">Image source</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/checklist.png\" style=\"height:50px;display:inline\"> Agenda\n",
    "---\n",
    "* [Machine Learning Overview](#-Machine-Learning-Overview)\n",
    "    * [What is Machine Learning?](#-What-is-Machine-Learning?)\n",
    "    * [Supervised Learning](#-Supervised-Learning)\n",
    "* [Discriminative vs Generative Models](#-Discriminative-vs-Generative-Models)\n",
    "    * [Discriminative Modelling](#-Discriminative-Modelling)\n",
    "    * [Generative Modelling](#-Generative-Modelling)\n",
    "* [Probabilistic Discriminative Models](#-Probabilistic-Discriminative-Models)\n",
    "    * [General Idea](#-General-Idea)\n",
    "    * [Binary Classification Example](#-Binary-Classification-Example)\n",
    "* [Recommended Videos](#-Recommended-Videos)\n",
    "* [Credits](#-Credits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## <img src=\"https://img.icons8.com/external-wanicon-lineal-color-wanicon/64/000000/external-machine-learning-smart-industry-wanicon-lineal-color-wanicon.png\" style=\"height:50px;display:inline\"> Machine Learning Overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/external-wanicon-lineal-color-wanicon/64/000000/external-machine-learning-smart-industry-wanicon-lineal-color-wanicon.png\" style=\"height:50px;display:inline\"> What is Machine Learning?\n",
    "---\n",
    "\n",
    "* Wikipedia definition: \"the study of computer algorithms that can improve automatically through experience and by the use of data\"\n",
    "* What does that really mean? Let's see an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* <u><b>Example 1</b></u>: Consider the task of predicting the duration of travelling the road given the current amount of cars.\n",
    "\n",
    "<img src=\"./assets/ml_tut_missing_number.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Naturally one can choose multiple models to predict the duration.\n",
    "* For example, we can limit ourselves to parametric models (eg polynomials)\n",
    "* Which of the following models should we choose?\n",
    "\n",
    "<img src=\"./assets/ml_tut_model_comp.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Higher polynomial degree ensures better fit to data.\n",
    "* However, better fit to data does not necessarily ensure better \"generalization\" error.\n",
    "* Assuming we have prior knowledge the function is smooth we might choose a $4^{th}$ order polynomial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/external-wanicon-lineal-color-wanicon/64/000000/external-machine-learning-smart-industry-wanicon-lineal-color-wanicon.png\" style=\"height:50px;display:inline\"> Data vs Prior Knowledge\n",
    "---\n",
    "\n",
    "* If we have a solid understanding of the system (geometry, physics, etc.), we rely mainly on prior knowledge.\n",
    "* However, if we do not have such understanding, we try to infer the system based of collected data.\n",
    "* For example, coming back to the example from earlier, if the car is driving at constant speed: duration = distance/speed. \n",
    "\n",
    "<img src=\"./assets/ml_tut_data_vs_prior.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* <u><b>Example 2</b></u>: Consider the task of classifying a credit card deal being legit or fraud based on some charachteristics like the Price, and the physical distance of the store from the card holder's home.\n",
    "\n",
    "<img src=\"./assets/ml_tut_transactions_dataset.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* We would like to create a predction function that tells us absed on these 2 features whether a credit card deal is legit or fraud.\n",
    "* For example here a naive implemnetation of such a function.\n",
    "\n",
    "<img src=\"./assets/ml_tut_transactions_example.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/external-wanicon-lineal-color-wanicon/64/000000/external-machine-learning-smart-industry-wanicon-lineal-color-wanicon.png\" style=\"height:50px;display:inline\"> Types of ML Setups\n",
    "---\n",
    "\n",
    "* There are 3 main different types of machine learning setups:\n",
    "    * <b> Supervised Learning </b> (This tutorial)\n",
    "    * Unsupervised Learning\n",
    "    * Reinforcement Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/external-wanicon-lineal-color-wanicon/64/000000/external-machine-learning-smart-industry-wanicon-lineal-color-wanicon.png\" style=\"height:50px;display:inline\"> What is Supervised Learning?\n",
    "---\n",
    "\n",
    "* Simplest form of machine learning (easiest to understand). Data is given in the form of examples with labels.\n",
    "* Algorithm is \"trained\" to predict the label for each example, while being given feedback for its answers.\n",
    "* when fully-trained the learning algorithm will be able to observe a new, never-before-seen example and predict a good label for it.\n",
    "* For example, label emails as spam, classify images as apples, classify cats vs dogs, predict location of tumor in medical images etc.\n",
    "\n",
    "<img src=\"./assets/ml_tut_supervised_learning.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/external-wanicon-lineal-color-wanicon/64/000000/external-machine-learning-smart-industry-wanicon-lineal-color-wanicon.png\" style=\"height:50px;display:inline\"> What is Unsupervised Learning?\n",
    "---\n",
    "\n",
    "* In this case we only have examples (data) with no labels.\n",
    "* Algorithm is \"trained\" to understand the properties of the data.\n",
    "* After training, it can learn to group, cluster, and/or organize the data in a way such that a human (or other intelligent algorithm) can come in and make sense of the newly organized data. (Example in lecture on K-means).\n",
    "\n",
    "<img src=\"./assets/ml_tut_unsupervised_learning.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/external-wanicon-lineal-color-wanicon/64/000000/external-machine-learning-smart-industry-wanicon-lineal-color-wanicon.png\" style=\"height:50px;display:inline\"> What is Reinforcement Learning?\n",
    "---\n",
    "\n",
    "* Fairly different from the previous two approaches, and beyond the scope of this course.\n",
    "* Essentially it tries to learn from mistakes with sparse/not-frequent supervision.\n",
    "* For example, learning a \"policy\" to navigate a maze with obstacles. \n",
    "\n",
    "<img src=\"./assets/ml_tut_reinforcement_learning.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/external-wanicon-lineal-color-wanicon/64/000000/external-supervised-business-administration-wanicon-lineal-color-wanicon.png\" style=\"height:50px;display:inline\"> Supervised Learning\n",
    "---\n",
    "\n",
    "* The basis of all other ML problems. Relates to estimation problems from statistical theory.\n",
    "* The estimation problem is the following: We want to predict the value of unknown random variable ($y$) based off other known random variables ($x$).\n",
    "* Usually in statistics, we assume the distribution of all random variables is known.\n",
    "* In Supervised learning, we assume we only have a finite sample from this distribution.\n",
    "* Therefore, our estimator will be built based off the finite sample only.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/external-wanicon-lineal-color-wanicon/64/000000/external-supervised-business-administration-wanicon-lineal-color-wanicon.png\" style=\"height:50px;display:inline\"> Supervised Learning - Notation\n",
    "---\n",
    "\n",
    "* Labels $y$ - The random variable we are trying to predict\n",
    "* Observations/Measurements $x$ - the random variables we are basing our predictor on.\n",
    "* Predictor/Estimator $\\hat{y} = h\\left(x\\right)$ - is the prediction function.\n",
    "* Dataset $\\mathcal{D} = \\left\\{x^{(i)},y^{(i)}\\right\\}_{i=1}^N$ - comprised of $N$ pairs of i.i.d samples from the joint distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/external-wanicon-lineal-color-wanicon/64/000000/external-supervised-business-administration-wanicon-lineal-color-wanicon.png\" style=\"height:50px;display:inline\"> Supervised Learning - Problem Types\n",
    "---\n",
    "\n",
    "* Depending on the values that $y$ can take we classify into 2 sub-types:\n",
    "* Continuous labels $y$ - a regression problem (estimating travel duration example).\n",
    "* Discrete labels $y$ - a classification problem (predicting legit vs fraud transaction example)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/external-wanicon-lineal-color-wanicon/64/000000/external-supervised-business-administration-wanicon-lineal-color-wanicon.png\" style=\"height:50px;display:inline\"> Supervised Learning - Regression\n",
    "---\n",
    "\n",
    "* Regression problem: $x$ = Number of cars, $y$ = Duration time:\n",
    "\n",
    "<img src=\"./assets/ml_tut_missing_number.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/external-wanicon-lineal-color-wanicon/64/000000/external-supervised-business-administration-wanicon-lineal-color-wanicon.png\" style=\"height:50px;display:inline\"> Supervised Learning - Classification\n",
    "---\n",
    "\n",
    "* Classification problem: $x$ = $\\left[\\text{Price}, \\text{Distance from home}\\right]^T$, $y \\in$ {Legit, Fraud}:\n",
    "\n",
    "<img src=\"./assets/ml_tut_transactions_dataset.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/external-wanicon-lineal-color-wanicon/64/000000/external-supervised-business-administration-wanicon-lineal-color-wanicon.png\" style=\"height:50px;display:inline\"> Supervised Learning - Optimal Estimator\n",
    "---\n",
    "\n",
    "* Any function mapping $h:x\\rightarrow y$ is a valid estimator.\n",
    "* Optimally we would like our estimator to make no mistakes.\n",
    "* Due to the labels ($y$) being a random variable, this is impossible.\n",
    "* Therefore, we need a way to compare the errors of different estimators to choose the best one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/external-wanicon-lineal-color-wanicon/64/000000/external-supervised-business-administration-wanicon-lineal-color-wanicon.png\" style=\"height:50px;display:inline\"> Supervised Learning - General Solution Paradigm\n",
    "---\n",
    "\n",
    "* Define a mathematical criterion that measures how good an estimator is doing\n",
    "* Choose a large enough family of models such that one of them will be good enough\n",
    "* Search all models in the chosen family for the best one.\n",
    "* (Easier said than done..)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/external-wanicon-lineal-color-wanicon/64/000000/external-supervised-business-administration-wanicon-lineal-color-wanicon.png\" style=\"height:50px;display:inline\"> Supervised Learning - Cost Function\n",
    "---\n",
    "\n",
    "* $C(h)$ Gives each estimator a score, lower score = better estimator.\n",
    "* Optimal estimator $h^* \\left(x\\right)$ is the one with the minimal score:\n",
    "$$h^* = \\underset{h}{argmin} \\ C(h)$$\n",
    "* Usually chosen from a subset of widely used functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/external-wanicon-lineal-color-wanicon/64/000000/external-supervised-business-administration-wanicon-lineal-color-wanicon.png\" style=\"height:50px;display:inline\"> Supervised Learning - Loss and Risk Functions\n",
    "---\n",
    "\n",
    "* The loss function $\\ell$ gives a score for a single prediction: $$\\ell(h(x),y) = \\ell(\\hat{y},y)$$\n",
    "* The cost is then defined as the expectation over the joint distribution: $$C(h) = \\mathbb{E}\\left[\\ell(h(x),y)\\right]$$\n",
    "* A familiar synonym for the cost function is the \"Risk\" function: $$R(h) \\equiv C(h)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/external-wanicon-lineal-color-wanicon/64/000000/external-supervised-business-administration-wanicon-lineal-color-wanicon.png\" style=\"height:50px;display:inline\"> Supervised Learning - Popular Loss/Risk Functions\n",
    "---\n",
    "\n",
    "* Classification Problems - Misclassification Rate: $$\\ell(\\hat{y},y) = I\\left\\{\\hat{y}\\neq y\\right\\}, \\ \\ R(h) = \\mathbb{E} \\left[I\\left\\{h(x)\\neq y\\right\\}\\right]$$\n",
    "\n",
    "* Optimal Classifier given by the Mode: $$ h^*(x) = \\underset{y}{argmax} \\ p(y|\\mathbf{x} = x) $$\n",
    "\n",
    "* Regression Problems - Mean Squared Error: $$\\ell(\\hat{y},y) = \\left(\\hat{y} - y\\right)^2, \\ \\ R(h) = \\mathbb{E} \\left[\\left(h(x) - y\\right)^2\\right]$$\n",
    "\n",
    "* Optimal Regressor given by the Conditional Mean: $$ h^*(x) = \\mathbb{E} \\left[y|\\mathbf{x} = x\\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/external-wanicon-lineal-color-wanicon/64/000000/external-supervised-business-administration-wanicon-lineal-color-wanicon.png\" style=\"height:50px;display:inline\"> Supervised Learning - Unknown Distribution and Empirical Risk\n",
    "---\n",
    "\n",
    "* Problem: We don't actually have access to the posterior distribution $p(y|x)$\n",
    "* Solution 1 (Generative Models): Estimate the joint distribution based on the dataset $\\mathcal{D} = \\left\\{x^{(i)},y^{(i)}\\right\\}_{i=1}^N$\n",
    "* Solution 2 (Discriminative Models): Estimate the expectation empirically based on the dataset $\\mathcal{D} = \\left\\{x^{(i)},y^{(i)}\\right\\}_{i=1}^N$\n",
    "* That is, replace the expectation with the empirical mean: $$R(h) = \\mathbb{E}\\left[\\ell(h(x),y)\\right] \\approx \\hat{R}(h) = \\hat{\\mathbb{E}}_{\\mathcal{D}}\\left[\\ell(h(x),y)\\right] = \\frac{1}{N} \\sum_{i=1}^{N}\\ell(h(x^{(i)}),y^{(i)})$$\n",
    "\n",
    "* Expected to converge in probability when $N \\rightarrow \\infty$\n",
    "* Using the empirical risk introduces the problem of overfitting (e.g. polynomial fitting example)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/external-wanicon-lineal-color-wanicon/64/000000/external-supervised-business-administration-wanicon-lineal-color-wanicon.png\" style=\"height:50px;display:inline\"> Supervised Learning - Performance Evaluation\n",
    "---\n",
    "\n",
    "* Goal: Learn a model from the given dataset that performs well on <b>unseen</b> data. \n",
    "* For that we would like to evaluate our estimator's performance on data not used in training.\n",
    "* This is usually achieved by splitting the dataset into two:\n",
    "    * Training Set - $\\mathcal{D}_{\\text{train}}$ - used to build our estimator $h^*(x)$\n",
    "    * Test Set - $\\mathcal{D}_{\\text{test}}$ - used to evaluate performance on new data\n",
    "* Performance on the test set can be approximated by the empirical mean: $$\\text{test score} = \\frac{1}{N_{\\text{test}}} \\sum_{x^{(i)}, y^{(i)} \\in \\mathcal{D}_{\\text{test}}} \\ell(h^*(x^{(i)}), y^{(i)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/external-wanicon-lineal-color-wanicon/64/000000/external-supervised-business-administration-wanicon-lineal-color-wanicon.png\" style=\"height:50px;display:inline\"> Supervised Learning - Parametric Models\n",
    "---\n",
    "\n",
    "* Usually, it is more practical to learn a parametric estimator $h(x;\\theta)$ with parameters $\\theta$, than searching the entire space of functions for a general $h(x)$.\n",
    "* This has the benefit of simplifying the optimization and usually reduces overfitting.\n",
    "* Examples of Parametric Functions:\n",
    "    * Linear functions: $h(x;\\theta) = \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 x_3$\n",
    "    * Polynomials: $h(x;\\theta) = \\theta_1 + \\theta_2 x_1 + \\theta_3 x_1^2 + \\theta_4 x_1^3$\n",
    "    * <b>Neural networks!</b> (Next Week)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/external-wanicon-lineal-color-wanicon/64/000000/external-supervised-business-administration-wanicon-lineal-color-wanicon.png\" style=\"height:50px;display:inline\"> Supervised Learning - Parametric Models Cont.\n",
    "---\n",
    "\n",
    "* Note that finding the best $h(x;\\theta)$ is now broadcasted to finding the optimal set of parameters $\\theta$.\n",
    "* This translates the minimization of the risk functions from earlier to rely on $\\theta$: \n",
    "$$h^* = \\underset{h}{argmin} \\ R(h) \\rightarrow \\theta^* = \\underset{\\theta}{argmin} \\ R(h(x;\\theta))$$\n",
    "* For the empirical risk case, this is approximated by:\n",
    "$$\\theta^* = \\underset{\\theta}{argmin} \\ \\frac{1}{N} \\sum_{i=1}^{N}\\ell(h(x^{(i)};\\theta),y^{(i)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <img src=\"https://img.icons8.com/color/96/000000/scatter-plot.png\" style=\"height:64px;display:inline\"> Discriminative vs Generative Models\n",
    "---\n",
    "\n",
    "* Recall: the problem is we don't have the posterior probability distribution $p(y|x)$\n",
    "* Solution 1 (Generative Models): Estimate the joint distribution based on the dataset $\\mathcal{D} = \\left\\{x^{(i)},y^{(i)}\\right\\}_{i=1}^N$\n",
    "* Solution 2 (Discriminative Models): Estimate the expectation empirically based on the dataset $\\mathcal{D} = \\left\\{x^{(i)},y^{(i)}\\right\\}_{i=1}^N$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/color/96/000000/scatter-plot.png\" style=\"height:64px;display:inline\"> Classification Example\n",
    "---\n",
    "\n",
    "* Let us get back to the classification example, and denote Legit as $y=0$, and Fraud as $y=1$.\n",
    "* First, we split the data into 80% training and 20% test:\n",
    "\n",
    "<img src=\"./assets/ml_tut_transactions_train_test.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://miro.medium.com/max/770/1*sY8G6iI6mr5mOy592A7WkQ.png\" style=\"height:64px;display:inline\"> Discriminative Modelling\n",
    "---\n",
    "\n",
    "* Main Idea - Build an estimator/classifier/discriminator from the empirical risk directly:\n",
    "$$ \\hat{h}_{\\mathcal{D}}(x) = \\underset{h}{argmin} \\ \\hat{\\mathbb{E}}_{\\mathcal{D}}\\left[\\ell(h(x),y)\\right] \\rightarrow \\hat{h}_{\\mathcal{D}}(x) = \\underset{h}{argmin} \\ \\frac{1}{N} \\sum_{i=1}^{N}\\ell(h(x^{(i)}),y^{(i)})$$\n",
    "\n",
    "* The estimator can be non-parametric ($h(x)$) or parameteric ($h(x;\\theta)$).\n",
    "* Here, we will discuss a non-parametric estimator (Nearest Neighbor)\n",
    "* In the lecture you will cover also a parametric estimator (Support Vector Machines)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://miro.medium.com/max/770/1*sY8G6iI6mr5mOy592A7WkQ.png\" style=\"height:64px;display:inline\"> Discriminative Modelling - Nearest Neighbor Classifier\n",
    "---\n",
    "\n",
    "* Algorithm That classifies based on the label of the nearest sample in the dataset:\n",
    "* We are given a dataset $\\mathcal{D} = \\left\\{x^{(i)},y^{(i)}\\right\\}_{i=1}^N$, and test sample $x_q$ that we want to predict the label for.\n",
    "* The algorithm is extremely simple and comprised of two steps:\n",
    "    * Find the the index of the nearest sample in the dataset: $$ i = \\underset{i}{argmin} \\| x_q - x^{(i)} \\|_2 $$\n",
    "    * Predict the label to be the label at the found index: $$\\hat{y} = y^{(i)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://miro.medium.com/max/770/1*sY8G6iI6mr5mOy592A7WkQ.png\" style=\"height:64px;display:inline\"> Discriminative Modelling - Nearest Neighbor Classifier Cont.\n",
    "---\n",
    "\n",
    "* Here are the resulting decision boundaries on the training set for the classification problem:\n",
    "\n",
    "<img src=\"./assets/ml_tut_nn1.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://miro.medium.com/max/770/1*sY8G6iI6mr5mOy592A7WkQ.png\" style=\"height:64px;display:inline\"> Discriminative Modelling - Nearest Neighbor Classifier Cont.\n",
    "---\n",
    "\n",
    "* The resulting error on the test score is 12%: $$\\text{test score} = \\frac{1}{N_{\\text{test}}} \\sum_{x^{(i)}, y^{(i)} \\in \\mathcal{D}_{\\text{test}}} \\ell(h^*(x^{(i)}), y^{(i)}) = 0.12$$\n",
    "\n",
    "<img src=\"./assets/ml_tut_nn1_test.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://miro.medium.com/max/770/1*sY8G6iI6mr5mOy592A7WkQ.png\" style=\"height:64px;display:inline\"> Discriminative Modelling - Nearest Neighbor Classifier Cont.\n",
    "---\n",
    "\n",
    "* The resulting test score can be reduced to 10% using a modified algorithm with $K=5$ neighbors (Details in the lecture): $$\\text{test score} = \\frac{1}{N_{\\text{test}}} \\sum_{x^{(i)}, y^{(i)} \\in \\mathcal{D}_{\\text{test}}} \\ell(h^*(x^{(i)}), y^{(i)}) = 0.1$$\n",
    "\n",
    "<img src=\"./assets/ml_tut_nn5_test.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://miro.medium.com/max/770/1*sY8G6iI6mr5mOy592A7WkQ.png\" style=\"height:64px;display:inline\"> Discriminative Modelling - Popular Algorithms\n",
    "---\n",
    "\n",
    "* K Nearest Neighbors\n",
    "* Decision Trees\n",
    "* Support Vector Machines\n",
    "* Linear Least Squares Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://miro.medium.com/max/630/1*Dl2yS5t0_9O_mG_BmHs2xw.png\" style=\"height:64px;display:inline\"> Generative Modelling\n",
    "---\n",
    "\n",
    "* First: Estimate the unknown joint distribution $\\hat{p}(x,y)$: Conviniently separated to $\\hat{p}(x|y)$ and $\\hat{p}(y)$\n",
    "* Second: Employ Bayes rule to get the approximated posterior (up to normalization): \n",
    "$\\hat{p}(y|x) = \\frac{\\hat{p}(x|y) \\hat{p}(y)}{p(x)} = \\frac{\\hat{p}(x|y) \\hat{p}(y)}{\\int_y{\\hat{p}(x|y) \\hat{p}(y)}}$\n",
    "* Third: Use the approximated posterior to get the final classifier: $ h^*(x) = \\underset{y}{argmax} \\ \\hat{p}(y|\\mathbf{x} = x)$ \n",
    "\n",
    "* The estimation of $p(x|y)$ can be non-parametric (KDE) or parameteric (eg Gaussians).\n",
    "* Here, we will discuss a parametric estimator for $p(x|y;\\theta)$ (Quadratic Discriminant Analysis)\n",
    "* In the lecture you will cover a non-parametric estimator $p(x|y)$ (Naive Bayes Classifier)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://miro.medium.com/max/630/1*Dl2yS5t0_9O_mG_BmHs2xw.png\" style=\"height:64px;display:inline\"> Generative Modelling - Quadratic Discriminant Analysis\n",
    "---\n",
    "\n",
    "* Assume we have a mixed joint probability distribution $p(x,y)$, where some of the variables are continuous $x$, and some are discrete $y$.\n",
    "* It is convinient in this case to write down $p(x,y) = p(x|y) p(y)$ and estimate each part separately:\n",
    "    * $p(y)$ can be simply estimated using label proportions in the dataset\n",
    "    * $p(x|y)$ can estimated separately for each value of $y$\n",
    "* If we further assume a parametric form $p(x|y;\\theta)$, we can turn the problem into estimating $\\theta$ per $y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://miro.medium.com/max/630/1*Dl2yS5t0_9O_mG_BmHs2xw.png\" style=\"height:64px;display:inline\"> Generative Modelling - Quadratic Discriminant Analysis Cont.\n",
    "---\n",
    "\n",
    "* Back to the example of the credit card transactions:\n",
    "* Training set have 200 data points with 160 Legit ($y=0$) and 40 Fraud ($y=1$).\n",
    "* Therefore $p(y)$ can be estimated using the proportions: $p(y=0) = \\frac{160}{200} = 0.8$ and $p(y=1) = \\frac{40}{200} = 0.2$\n",
    "\n",
    "<img src=\"./assets/ml_tut_transactions_train.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://miro.medium.com/max/630/1*Dl2yS5t0_9O_mG_BmHs2xw.png\" style=\"height:64px;display:inline\"> Generative Modelling - Quadratic Discriminant Analysis Cont.\n",
    "---\n",
    "\n",
    "* Next, we estimate a conditional density $p(x|y)$ for each $y$ separately: $p(x|y=0)$ and $p(x|y=1)$.\n",
    "* This can done with a non-parametric method (eg KDE) or a parametric one (eg Gaussian density).\n",
    "* For the parameteric case, we can derive the optimal parameters using Maximum Likelihood Estimation (MLE).\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"./assets/ml_tut_transactions_train_y0.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://miro.medium.com/max/630/1*Dl2yS5t0_9O_mG_BmHs2xw.png\" style=\"height:64px;display:inline\"> Generative Modelling - Quadratic Discriminant Analysis Cont.\n",
    "---\n",
    "\n",
    "* Assuming a parametric Gaussian density, then for each $p(x|y)$ we will estimate an expectation vector $\\mu_{x|y}$ and a covariance matrix $\\Sigma_x|y$.\n",
    "* The resulting estimated posterior probabilities are given by:\n",
    "$$ \\hat{p}(y=0|x) = \\frac{\\hat{p}(x|y=0)\\hat{p}(y=0)}{p(x)}, \\ \\hat{p}(y=1|x) = \\frac{\\hat{p}(x|y=1)\\hat{p}(y=1)}{p(x)} $$\n",
    "* Finally the resulting estimator is given by the mode of the resulting posterior using Bayes rule:\n",
    "$$h^*(x) = \\underset{y}{argmax} \\ \\hat{p}(y|\\mathbf{x} = x) = \\underset{y}{argmax} \\left\\{\\hat{p}(y=0|x), \\hat{p}(y=1|x)\\right\\}$$\n",
    "* This translates to the criterion:\n",
    "$$ \n",
    "h^*(x) = \\begin{cases}\n",
    "    0 & \\text{if } \\hat{p}(x|y=0)\\hat{p}(y=0) > \\hat{p}(x|y=1)\\hat{p}(y=1) \\\\ % & is your \"\\tab\"-like command (it's a tab alignment character)\n",
    "    1 & \\text{otherwise.}\n",
    "\\end{cases}\n",
    "$$\n",
    "* Substituting the Gaussian distribution in $p(x|y=0/1)$ results in quadratic boundaries, hence the name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://miro.medium.com/max/630/1*Dl2yS5t0_9O_mG_BmHs2xw.png\" style=\"height:64px;display:inline\"> Generative Modelling - Quadratic Discriminant Analysis Cont.\n",
    "---\n",
    "\n",
    "* Note that the orange Gaussian is probably a poor model choice and a mixture model of two Gaussians would have worked much better.\n",
    "* The resulting test score is 8%: $$\\text{test score} = \\frac{1}{N_{\\text{test}}} \\sum_{x^{(i)}, y^{(i)} \\in \\mathcal{D}_{\\text{test}}} \\ell(h^*(x^{(i)}), y^{(i)}) = 0.08$$\n",
    "\n",
    "<img src=\"./assets/ml_tut_transactions_qda.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://miro.medium.com/max/630/1*Dl2yS5t0_9O_mG_BmHs2xw.png\" style=\"height:64px;display:inline\"> Generative Modelling - Popular Algorithms\n",
    "---\n",
    "\n",
    "* Linear Discriminant Analysis\n",
    "* Quadratic Discriminant analysis\n",
    "* Naive Bayes\n",
    "* Markov Random Fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://miro.medium.com/max/630/1*Dl2yS5t0_9O_mG_BmHs2xw.png\" style=\"height:64px;display:inline\"> Generative Modelling - Limitations\n",
    "---\n",
    "\n",
    "* Suffers from the curse of dimensionality:\n",
    "    * Space coverage becomes increasly more difficult for higher dimensions of $x$\n",
    "    * Number of samples $n$ needed to estimate the conditional density $p(x|y)$ is exponential in the dimension: $\\approx n^{d}$\n",
    "    * Naive solution via assuming independence (Naive Bayes Classifier)\n",
    "* Models we can work with are very limited, because we need to satisfy:\n",
    "    * $p(x,y;\\theta) \\geq 0, \\ \\forall x,y,\\theta$\n",
    "    * $\\int \\int {p(x,y;\\theta)} = 1, \\ \\forall \\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <img src=\"https://img.icons8.com/external-prettycons-lineal-prettycons/49/000000/external-function-education-prettycons-lineal-prettycons.png\" style=\"height:40px;display:inline\"> Probabilistic Discriminative Models\n",
    "---\n",
    "\n",
    "* Recall: the problem is we don't have the posterior probability distribution $p(y|x)$\n",
    "* Solution 1 (Generative Models): Estimate the <b>joint</b> distribution based on the dataset $\\mathcal{D} = \\left\\{x^{(i)},y^{(i)}\\right\\}_{i=1}^N$\n",
    "* Solution 2 (Discriminative Models): Estimate the expectation empirically based on the dataset $\\mathcal{D} = \\left\\{x^{(i)},y^{(i)}\\right\\}_{i=1}^N$\n",
    "* <font color='red'>Solution 3 (Prob. Discriminative Models)</font>: Estimate the <b>posterior</b> distribution <b>directly</b> based on the dataset $\\mathcal{D} = \\left\\{x^{(i)},y^{(i)}\\right\\}_{i=1}^N$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/external-prettycons-lineal-prettycons/49/000000/external-function-education-prettycons-lineal-prettycons.png\" style=\"height:40px;display:inline\"> Probabilistic Discriminative Models -General Idea\n",
    "---\n",
    "\n",
    "* Usually these methods in most books will be called simply discriminative without the \"probabilistic\".\n",
    "* However, most good books will still differentiate between different types of discriminative models although without special names.\n",
    "* Here, we will estimate directly $p(y|x)$, usually by parameterizing it to $p(y|x;\\theta)$.\n",
    "* Parameter estimation as usual will be done with maximum likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/external-prettycons-lineal-prettycons/49/000000/external-function-education-prettycons-lineal-prettycons.png\" style=\"height:40px;display:inline\"> Probabilistic Discriminative Models -General Idea Cont.\n",
    "---\n",
    "\n",
    "* For classification problems, the function $p(y|x;\\theta)$ should satisfy:\n",
    "    * $p(y|x;\\theta) \\geq 0, \\ \\forall x,y,\\theta$\n",
    "    * $\\sum_{y=1}^C {p(y|x;\\theta)} = 1, \\ \\forall x,\\theta$\n",
    "* The second here is much simpler than the condition for generative models which was:\n",
    "    $\\int \\int {p(x,y;\\theta)} = 1, \\ \\forall \\theta$\n",
    "* Such models can be easily constructed, for example for $C=2$ classes (binary classification), we only demand: $$p(y=0|x;\\theta) + p(y=1|x;\\theta) = 1 \\ \\forall x,\\theta$$\n",
    "* Any parametric function $f(x;\\theta)$ that returns values between 0 and 1 can define a valid model like so:\n",
    "$$ \n",
    "p(y=1|x) = f(x;\\theta) \\\\\n",
    "p(y=0|x) = 1 - f(x;\\theta)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/external-prettycons-lineal-prettycons/49/000000/external-function-education-prettycons-lineal-prettycons.png\" style=\"height:40px;display:inline\"> Probabilistic Discriminative Models - Binary Classification Example\n",
    "---\n",
    "\n",
    "* Assume we are dealing with a binary classification problem $y \\in \\{0,1\\}$.\n",
    "* Using the sigmoid function we can even relax the range condition of $f(x;\\theta)$.\n",
    "* Known as Logistic Regression in the literature.\n",
    "* Any parametric model of the following form is valid:\n",
    "$$ \n",
    "p(y=1|x) = \\sigma(f(x;\\theta)) \\\\\n",
    "p(y=0|x) = 1 - \\sigma(f(x;\\theta))\n",
    "$$\n",
    "\n",
    "<img src=\"./assets/ml_tut_sigmoid.png\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/external-prettycons-lineal-prettycons/49/000000/external-function-education-prettycons-lineal-prettycons.png\" style=\"height:40px;display:inline\"> Probabilistic Discriminative Models - Binary Classification Example\n",
    "---\n",
    "\n",
    "* Estimating the parameters $\\theta$ with MLE we get the following objective function:\n",
    "$$\\begin{aligned}\n",
    "\\theta^*&=\\underset{\\theta}{\\text{argmin}} -\\sum_{i=1}^N \\log(p(y^{(i)}|x^{(i)};\\theta))\\\\\n",
    "&=\\underset{\\theta}{\\text{argmin}} -\\sum_{i=1}^N I\\left\\{y^{(i)}=1\\right\\} \\log(\\sigma(f(x^{(i)};\\theta)) + I\\left\\{y^{(i)}=0\\right\\} \\log(1 - \\sigma(f(x^{(i)};\\theta))\\\\\n",
    "&=\\underset{\\theta}{\\text{argmin}} -\\sum_{i=1}^N y^{(i)}\\log(\\sigma(f(x^{(i)};\\theta)) + (1-y^{(i)}) \\log(1 - \\sigma(f(x^{(i)};\\theta))\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "* Resulting estimator after optimization:\n",
    "$$h(x) = \\underset{y}{\\text{argmax}} p(y|x;\\theta) = \n",
    "\\begin{cases}\n",
    "    1 & \\text{if } \\sigma(f(x;\\theta)) > 0.5 \\\\ % & is your \"\\tab\"-like command (it's a tab alignment character)\n",
    "    0 & \\text{otherwise.}\n",
    "\\end{cases} = \n",
    "\\begin{cases}\n",
    "    1 & \\text{if } f(x;\\theta) > 0 \\\\ % & is your \"\\tab\"-like command (it's a tab alignment character)\n",
    "    0 & \\text{otherwise.}\n",
    "\\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/external-prettycons-lineal-prettycons/49/000000/external-function-education-prettycons-lineal-prettycons.png\" style=\"height:40px;display:inline\"> Probabilistic Discriminative Models - Binary Classification Example\n",
    "---\n",
    "\n",
    "* Coming back to the fraud example from earlier (taking only one part of the space for simplicity):\n",
    "\n",
    "<img src=\"./assets/ml_tut_transactions_half_data.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/external-prettycons-lineal-prettycons/49/000000/external-function-education-prettycons-lineal-prettycons.png\" style=\"height:40px;display:inline\"> Probabilistic Discriminative Models - Binary Classification Example\n",
    "---\n",
    "\n",
    "* Fitting a linear parametric function: $ f(x;\\theta) = \\theta^T x $\n",
    "* The resulting test score is 2%: $$\\text{test score} = \\frac{1}{N_{\\text{test}}} \\sum_{x^{(i)}, y^{(i)} \\in \\mathcal{D}_{\\text{test}}} \\ell(h^*(x^{(i)}), y^{(i)}) = 0.02$$\n",
    "\n",
    "\n",
    "<img src=\"./assets/ml_tut_transactions_logistic_linear.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/external-prettycons-lineal-prettycons/49/000000/external-function-education-prettycons-lineal-prettycons.png\" style=\"height:40px;display:inline\"> Probabilistic Discriminative Models - Binary Classification Example\n",
    "---\n",
    "\n",
    "* For $f(x;\\theta)$ taken as a second order polynomial, the resulting test score is 0% (no mistakes!): $$\\text{test score} = \\frac{1}{N_{\\text{test}}} \\sum_{x^{(i)}, y^{(i)} \\in \\mathcal{D}_{\\text{test}}} \\ell(h^*(x^{(i)}), y^{(i)}) = 0$$\n",
    "\n",
    "\n",
    "<img src=\"./assets/ml_tut_transactions_logistic_poly.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/video-playlist.png\" style=\"height:50px;display:inline\"> Recommended Videos\n",
    "---\n",
    "#### <img src=\"https://img.icons8.com/cute-clipart/64/000000/warning-shield.png\" style=\"height:30px;display:inline\"> Warning!\n",
    "* These videos do not replace the lectures and tutorials.\n",
    "* Please use these to get a better understanding of the material, and not as an alternative to the written material.\n",
    "\n",
    "#### Video By Subject\n",
    "* Machine Learning Course by Andrew Ng -  <a href=\"https://www.coursera.org/learn/machine-learning\"> Coursera </a>\n",
    "* Lectures 1-7, EE 046195 by Omer Yair -  <a href=\"https://moodle.technion.ac.il/course/view.php?id=1776\"> Technion </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <img src=\"https://img.icons8.com/dusk/64/000000/prize.png\" style=\"height:50px;display:inline\"> Credits\n",
    "----\n",
    "* EE 046746 Spring 2021 - <a href=\"https://eliasnehme.github.io/\">Elias Nehme</a>\n",
    "* Lectures 1-7, EE 046195 Spring 2021 - <a href=\"https://technion046195.netlify.app/\">Omer Yair</a>\n",
    "* What are the Types of Machine Learning? - <a href=\"https://towardsdatascience.com/what-are-the-types-of-machine-learning-e2b9e5d1756f\">Hunter Heidenreich</a>\n",
    "\n",
    "* Icons from <a href=\"https://icons8.com/\">Icon8.com</a> - https://icons8.com"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "rise": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
